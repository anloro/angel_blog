---
layout: "wrapper"
title:  "Optimal estimation in dynamic systems: Parameter estimators"
categories: OEDS
---

Parameter estimation is the process of guessing the values of a certain object basing the estimations on measurements obtained from that same object. It can be an object, a process or an event. The statement of a parameter estimation problem would be as follows: Given a dataset of n points z = \{z[0], z[1], ... ,z[n-1]\} which is dependent of an unknown parameter x ùúñ ‚Ñù, we want to create an estimator for x, represented as *xÃÇ*. With the form:


| *xÃÇ* = g(z[0], z[1], ... ,z[n-1])

Notation: 
* x: Unknown parameter to estimate.
* *xÃÇ*: Parameter estimator.
* g: Function that approximates the parameter values based on the measurements.
* z: Measurements of the process, object or event. 


## Bayesian estimation 
Bayesian estimation is a part of Bayesian statistics which tryes to minimize the posterior expected value of a cost function [link](https://en.wikipedia.org/wiki/Bayes_estimator). In Bayesian estimation the parameters are considered as random variables instead of unknown constants.

The optimization criterion for Bayesian estimation is to obtain minimum risk (which will be defined later), but first two conditions must be met:  
1. It must be possible to quatify the cost when the estimations differ from the true parameter.
2. The expectation of the cost (the risk) should be acceptable as an optimization criterion. 

There are several cost functions, but in most applications it is difficult to obtain it, and therefore simple functions are used: 

**Quadratic cost function**: 

| C(*xÃÇ* \| x) = \|\|*xÃÇ* - x\|\|<sub>2</sub><sup>2</sup> = <span><summatory idb="N-1" id="n = 0">&Sigma;</summatory> (*xÃÇ*<sub>n</sub> - x<sub>n</sub>)<sup>2</sup> </span>

**Absolute value cost function**:

| C(*xÃÇ* \| x) = \|\|*xÃÇ* - x\|\|<sub>1</sub> = <span><summatory idb="N-1" id="n = 0">&Sigma;</summatory> \|*xÃÇ*<sub>n</sub> - x<sub>n</sub>\|<sup>2</sup> </span>

**Uniform cost function**:
<table>
<td>
    <table class="inline">
    <td>
    C(xÃÇ | x) = 
    </td>
    <td>
        <blockquote>
        <table class="inline">
            <tr>
                <td>1 if ||xÃÇ - x||<sub>1</sub> > &Delta;</td>
            </tr>
            <tr>
                <td>0 if ||xÃÇ - x||<sub>1</sub> ‚â§ &Delta;</td>
            </tr>
        </table>
        </blockquote>
    </td>
    <td>
    where &Delta; ‚Üí 0 
    </td>
    </table>
</td>
</table>

Notation:
* C: Cost.
* ÃÉx: estimation error. 

The risk is the expectation of the cost, so we can define the **conditional risk** as:

| R(*xÃÇ* \| z) = E[ C(*xÃÇ* \| x) \| z] = <span style= "font-size:20px;">&#8747;</span> C(*xÃÇ* \| x) p(x \| x)dx

The **optimal estimator** is the one that minimized the risk so it is defined as:

| *xÃÇ*(z) = argmin{R(x \| z)}

We can also define the **overall risk** as the expected cost seen over the full set of possible measurements:

| R = E[R(*xÃÇ* \| z)] = <span style= "font-size:20px;">&#8747;</span> R(*xÃÇ* \| z) p(z)dz

As p(z) is always positive, we can get the minimum overall risk by minimizing the conditional risk. For optaining the optimal solution we only have to differentiate and equate to zero.

By substituting the different cost functions in the previous formulas and finding the optimal we can obtain different optimal estimators *xÃÇ*: MMSE (Minimum Mean Square Error), MMAE (Minimum Mean Absolute Error) and MAP (Maximum A Posteriori).

Solutions in the scalar case:

|Abreviation: | **MMSE** | **MMAE** | **MAP**
|Estimator: | *xÃÇ*<sub>MMSE</sub>(z) = E[x\|z] | *xÃÇ*<sub>MMAE</sub>(z) = median[p(x\|z)] | *xÃÇ*<sub>MAP</sub>(z) = argmax{p(x\|z)}
|Cost function that optimizes: | Quadratic cost | Absolute cost | Uniform cost

There are some practical situations where there is no prior knowledge available, in this case we still can have an estimator without any prior distribution modifying the MAP estimatior, now named ML (Maximum likelihood) estimator. 

| *xÃÇ*<sub>MAP</sub>(z) = argmax{p(z\|x)}
